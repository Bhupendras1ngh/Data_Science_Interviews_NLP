Category,Questions,Answers
Model,assumption linear regression assumption violated,four assumption associated linear regression linearity relationship mean linear homoscedasticity variance residual independence observation independent normality fixed normally distributed extreme violation assumption redundant violation assumption result greater bias variance estimate
Statistics,collinearity multicollinearity deal,collinearity linear association predictor multicollinearity situation predictor highly linearly problematic undermines statistical significance independent variable necessarily impact accuracy affect variance prediction reduces interpretation independent variable variance inflation factor vif determine multicollinearity independent variable standard benchmark vif greater multicollinearity exists
Model,drawback linear,couple drawback linear linear hold strong assumption true application assumes linear relationship multivariate normality multicollinearity auto correlation homoscedasticity linear discrete binary outcome vary flexibility linear
Model,ridge lasso regression difference,regularization method reduce overfitting least square minimizes sum squared residual result bias variance regularization called ridge regression minimizes sum squared residual plus lambda slope squared additional term called ridge regression penalty increase bias making fit worse decrease variance ridge regression penalty replace absolute slope lasso regression regularization le robust stable solution always solution robust unstable solution possibly multiple solution
Model,nearest neighbor,nearest neighbor classification technique sample classified looking nearest classified point hence nearest example above unclassified classified blue outlier overlook class few sample
Model,mean,elbow method popular method determine optimal essentially plot squared error graph axis squared error axis once graph distortion decline elbow
Model,naive bayes naive,naive bayes naive hold strong assumption assumed uncorrelated typically never
Model,vector svm,vector point touch boundary maximum margin
Model,pruning decision tree,pruning technique machine learning algorithm reduces decision tree removing section branch tree classifying instance
Model,random forest naive bayes,random forest ensemble learning technique build decision tree random forest involve creating multiple decision tree bootstrapped datasets original randomly selecting subset variable step decision tree selects mode prediction decision tree relying majority win reduces risk error individual tree random forest offer several benefit strong performance linear boundary cross validation needed give feature importance naive bayes sense easy train understand random forest seem therefore naive bayes algorithm implementation understanding performance random forest typically stronger ensemble technique
Model,random forest v svm,couple reason random forest choice algorithm vector machine random forest allow determine feature importance svm random forest quicker simpler build svm multi classification problem svms require v rest method le scalable memory intensive
Model,decision tree,asking question random forest decision tree answer random forest ensemble method take weak decision tree strong learner random forest accurate robust le prone overfitting
Model,difference adaboosted tree gradient boosted tree,adaboost boosted algorithm similar random forest couple significant difference rather forest tree adaboost typically make forest stump stump tree node leaf stump decision weighted equally final decision stump le error accuracy higher stump created important subsequent stump emphasizes importance sample incorrectly classified stump gradient boost similar adaboost sense build multiple tree tree built tree unlike adaboost build stump gradient boost build tree usually leaf importantly gradient differs adaboost decision tree built gradient boost start initial prediction usually average decision tree built residual sample prediction taking initial prediction learning outcome residual tree repeated
Statistics,bias variance tradeoff,bias estimator difference expected true bias tends oversimplified underfitting variance represents sensitivity noise variance overfitting therefore bias variance tradeoff machine learning model lower variance higher bias vice versa generally optimal balance error minimized
EDA,explain bootstrap sampling method give example,technically speaking bootstrap sampling method resampling method us random sampling replacement essential random forest algorithm ensemble learning algorithm
Methodology,difference bagging boosting,bagging known bootstrap aggregating multiple model learning algorithm trained bootstrapped sample original dataset random forest example above vote taken model output boosting variation bagging individual built sequentially iterating specifically point falsely classified emphasized done improve overall accuracy once built falsely classified predicted point taken addition bootstrapped sample train ensemble model against test dataset continues
Model,xgboost handle bias variance tradeoff,xgboost ensemble machine learning algorithm leverage gradient boosting algorithm essence xgboost bagging boosting technique steroid therefore xgboost handle bias variance similar boosting technique boosting ensemble meta algorithm reduces bias variance take weighted average weak model focusing weak prediction iterating model error thus bias reduced similarly take weighted average weak model final lower variance weaker model themselves
Methodology,cross validation,cross validation essentially technique ass performs independent dataset simplest example cross validation split group validation testing build validation tune hyperparameters testing evaluate final
Methodology,difference batch learning,batch learning known offline learning group pattern learning familiar dataset build whole dataset once learning hand approach ingests observation learning efficient longer once consumed technically mean don
EDA,give several way deal missing value,way handle null value omit row null value altogether replace null value measure central tendency mean median mode replace eg none predict null value variable example row null weight height replace null average weight given height lastly leave null value machine learning automatically deal null value
EDA,mean imputation missing acceptable practice,mean imputation practice replacing null value mean mean imputation generally bad practice doesn feature correlation example imagine showing age fitness score imagine eighty missing fitness score took average fitness score age range eighty appear higher fitness score actually mean imputation reduces variance increase bias lead le accurate narrower confidence interval due smaller variance
Methodology,confusion matrix,confusion matrix known error matrix summarized ass performance classification correct incorrect prediction summarized count value broken
Methodology,supervised v unsupervised learning,supervised learning involves learning labeled dataset target variable known unsupervised learning draw inference pattern input reference labeled outcome target variable
Methodology,ensemble learning,ensemble learning method multiple learning algorithm conjunction purpose doing allows achieve higher predictive performance individual algorithm itself example random forest
EDA,identify outlier,couple way identify outlier z score standard deviation lie standard deviation calculate standard deviation multiply identify point outside range likewise calculate z score given equal outlier few contingency considered method normally distributed applicable set presence outlier throw z score interquartile range iqr iqr concept build boxplots identify outlier iqr equal difference rd quartile st quartile identify outlier le q irq greater q iqr come approximately standard deviation method dbscan clustering isolation forest robust random cut forest
EDA,inlier,inlier observation lie rest dataset unusual error lie dataset typically harder identify outlier requires external identify
EDA,outlier treated,couple way remove outlier garbage try example linear might treat outlier differently linear normalize narrow range algorithm outlier random forest
Methodology,collaborative filtering filtering similar,filtering property object similar example filtering recommender recommend genre directed director collaborative filtering behavior compared user user similar behavior dictate recommended give simple example bought bought recliner recommended recliner
Methodology,principal component analysis explain sort problem pca,simplest sense pca involves higher dimensional eg dimension smaller space eg dimension lower dimension dimension instead dimension keeping original variable pca commonly compression purpose reduce memory speed algorithm visualization purpose making easier summarize
EDA,difference validation test,generally validation tune hyperparameters testing evaluate final
Methodology,avoid overfitting,don overfitting modeling error function fit closely resulting level error introduced way prevent overfitting cross validation cross validation technique ass performs independent dataset simplest example cross validation split group testing build testing test regularization overfitting occurs model higher degree polynomial thus regularization reduces overfitting penalizing higher degree polynomial reduce reduce overfitting simply reducing input manually removing technique called principal component analysis project higher dimensional eg dimension smaller space eg dimension ensemble learning technique ensemble technique weak learner convert strong learner bagging boosting bagging boosting technique tend overfit le alternative counterpart
EDA,step wrangling cleaning applying machine learning algorithm,step taken wrangling cleaning common step listed below profiling almost everyone start getting understanding dataset specifically shape dataset shape numerical variable describe visualization sometimes useful visualize histogram boxplots scatterplots understand relationship variable identify potential outlier syntax error includes making sure space making sure letter casing consistent checking typo typo unique bar graph standardization normalization depending dataset working machine learning method decide useful standardize normalize scale variable don negatively impact performance handling null value way handle null value deleting row null value altogether replacing null value mean median mode replacing null value eg unknown predicting value machine learning model deal null value thing removing irrelevant removing duplicate conversion
Methodology,deal unbalanced binary classification,way handle unbalanced binary classification assuming identify minority reconsider metric evaluate accuracy might metric ll example explain let bank withdrawal fraudulent withdrawal simply classified instance fraudulent accuracy therefore consider metric precision recall method improve unbalanced binary classification increasing misclassifying minority increasing penalty classify minority accurately lastly improve balance class oversampling minority undersampling majority
Statistics,difference precision recall,recall attempt answer proportion actual positive identified correctly precision attempt answer proportion positive identification actually correct
Methodology,mean square error bad measure performance suggest instead,mean squared error mse give relatively weight error therefore mse tends put emphasis deviation robust alternative mae mean absolute deviation
Statistics,explain false positive false negative important example false positive important false negative false negative important false positive,false positive incorrect identification presence condition absent false negative incorrect identification absence condition actually present example false negative important false positive screening cancer worse someone doesn cancer instead saying someone later realizing don subjective argument false positive worse false negative psychological example false positive winning lottery worse outcome false negative normally don expect win lottery anyway
EDA,feature selection method variable,type method feature selection filter method wrapper method filter method linear discrimination analysis anova chi square wrapper method forward selection test feature keep adding until fit backward selection test removing work
Model,briefly explain basic neural work,core neural essentially mathematical equation take input variable equation output variable neural input layer hidden layer output layer input layer consists feature variable input variable independent variable denoted xn hidden layer consists hidden node hidden unit similarly output variable consists output unit beginning neural nothing equation node neural composed function linear function activation function thing confusing linear function fit activation function light switch
Model,rectified linear unit activation function,rectified linear unit known relu function known activation function sigmoid function tanh function performs gradient descent faster
Model,weight initialized,weight neural initialized randomly expectation stochastic gradient descent initialized weight zero hidden unit exactly signal example weight initialized hidden unit zero signal
Model,happens learning,learning train slowly minimal update weight iteration thus update reaching minimum learning cause undesirable divergent behavior loss function due drastic update weight fail converge
Model,recurrent neural network,recurrent neural network known rnns neural network allow output input having hidden commonly recognize pattern sequence etc
Model,role activation function,purpose activation function introduce linearity output neuron activation function decides whether neuron activated calculating weighted sum further adding bias
Statistics,defined,probability obtaining observed test assuming null hypothesis correct smaller mean stronger evidence favor alternative hypothesis
Statistics,covariance correlation,covariance quantitative measure extent deviation variable mean match deviation mean correlation measurement relationship variable covariance variable normalized variance variable
Statistics,number,number theory trial increase average result become closer expected eg flipping head fair coin closer
Statistics,central limit theorem explain important,central limit theorem sampling distribution sample mean approach normal distribution sample get larger matter shape population distribution central limit theorem important hypothesis testing calculate confidence interval
Statistics,markov,modeling stochastic agent make random decision assumption referred markov
Statistics,statistical,statistical refers binary hypothesis probability test reject null hypothesis given alternative hypothesis true
Statistics,confounding variable,confounding variable confounder variable influence dependent variable independent variable causing spurious association mathematical relationship variable associated causally
Statistics,experimental contrast observational,observational come observational study observe certain variable intervening try determine correlation experimental come experimental study intervention certain variable hold constant determine causality
Statistics,explain selection bias regard dataset variable selection important procedure missing handling worse,selection bias phenomenon selecting individual group analysis proper randomization achieved ultimately resulting sample representative population understanding identifying selection bias important significantly skew false insight particular population type selection bias sampling bias biased sample caused random sampling interval selecting specific frame support desired conclusion conducting analysis near christmas exposure includes clinical susceptibility bias protopathic bias indication bias includes cherry picking suppressing evidence fallacy incomplete evidence attrition attrition bias similar survivorship bias survived included analysis failure bias failed included observer selection anthropic principle philosophical consideration collect universe filtered fact observable compatible conscious sapient observes handling missing selection bias worse method impact way example replace null value mean adding bias sense assuming spread might actually
Statistics,difference interpolation extrapolation matter,interpolation prediction input lie observed value extrapolation prediction input outside observed value important distinction interpolation generally accurate extrapolation
Statistics,give example median measure mean,outlier positively negatively skew
Statistics,survivorship bias,phenomenon survived included excluded analysis thus creating biased sample example sreenivasan chandrasekar enroll gym membership attend few face fit motivated exercising everyday whenever gym few become depressed aren able stick schedule motivation saw gym didn enrolled gym membership stopped turning gym didn
Statistics,root cause analysis identify cause v correlation give example,root cause analysis method problem solving identifying root cause problem identify correlation simple analysis identify causation conducting experiment variable isolated ideally
Statistics,give type statistical bias explain example,sampling bias refers biased sample caused random sampling give example imagine ask prefer grape banana surveyed female concluded majority grape demonstrated sampling bias confirmation bias tendency favour confirms belief survivorship bias phenomenon survived included excluded analysis thus creating biased sample
Statistics,explain tailed distribution example relevant phenomenon tail important classification regression problem,tailed distribution heavy tailed distribution tail tail drop gradually asymptotically practical example pareto principle commonly known rule selling v others important mindful tailed distribution classification regression problem least frequently occurring value majority population ultimately deal outlier conflict machine learning technique assumption normally distributed
Statistics,testing practice,testing statistical hypothesis testing meant randomized experiment variable commonly marketing
Statistics,bias,thing minimize bias common thing randomization participant assigned chance random sampling sampling equal probability chosen
Statistics,give example gaussian distribution nor log normal,categorical won gaussian distribution lognormal distribution exponential distribution eg amount battery last amount until earthquake occurs
Statistics,ass statistical significance insight,perform hypothesis testing determine statistical significance null hypothesis alternative hypothesis calculate probability obtaining observed test assuming null hypothesis true significance alpha le alpha reject null word result statistically significant
Statistics,homicide scotland fell reported noteworthy,poisson distribution question mean lambda variance mean standard deviation square root mean confidence interval implies z score standard deviation sqrt therefore confidence interval confidence interval assume noteworthy
Statistics,difference boxplot histogram,boxplots histogram visualization distribution communicate differently histogram bar chart frequency numerical variable value approximate probability distribution given variable allows quickly understand shape distribution variation potential outlier boxplots communicate aspect distribution shape distribution plot gather quartile range outlier boxplots especially useful multiple chart le space histogram
Statistics,meaning acf pacf,understand acf pacf autocorrelation serial correlation autocorrelation look degree similarity given lagged itself therefore autocorrelation function acf tool pattern specifically correlation point separated various lag example acf mean point perfectly correlated themselves acf mean correlation pacf short partial autocorrelation function quoting stackexchange thought correlation point separated period effect intervening correlation removed example directly correlated directly correlated appear correlated pacf remove intervening correlation
Methodology,experiment feature thinking metric matter,conduct test determine introduction feature statistically significant improvement given metric metric chosen depends goal feature example feature introduced increase conversion rate traffic retention rate formulate null hypothesis feature improve metric alternative hypothesis feature improve metric create test random sampling test inherently considers sample specify necessary sample although larger once collect depending characteristic conduct test welch test chi squared test bayesian test determine whether difference test statistically significant
Methodology,prove male average taller female knowing gender height,hypothesis testing prove male taller average female null hypothesis male female height average alternative hypothesis average height male greater average height female collect random sample height male female test determine reject null
Methodology,say double ad newsfeed figure idea,perform test splitting user group normal ad test double ad choose metric define idea example null hypothesis doubling ad reduce spent facebook alternative hypothesis doubling ad won impact spent facebook choose metric active user churn conduct test determine statistical significance test reject reject null
Methodology,tell given coin biased,isn trick question answer simply perform hypothesis test null hypothesis coin biased probability flipping head equal alternative hypothesis coin biased flip coin calculate z score sample le calculate statistic against alpha tailed test alpha null rejected coin biased alpha null rejected coin biased
Methodology,define metric,isn fit metric metric chosen evaluate machine learning depends various factor regression classification task objective eg precision v recall distribution target variable metric adjusted squared mae mse accuracy recall precision score go
Methodology,dimension reduction important,dimensionality reduction reducing dataset important mainly reduce variance overfitting wikipedia four advantage dimensionality reduction reduces storage space removal multi collinearity improves interpretation parameter machine learning becomes easier visualize reduced dimension avoids curse dimensionality
Model,naive bayes bad improve spam detection algorithm us naive bayes,major drawback naive bayes hold strong assumption assumed uncorrelated typically never improve algorithm us naive bayes decorrelating assumption hold true
Model,regression fit,couple metric squared adjusted squared relative measure fit explained answer score evaluates null hypothesis regression coefficient equal zero v alternative hypothesis least doesn equal zero rmse absolute measure fit
Model,decision tree,decision tree popular operation strategic planning machine learning square above called node node accurate decision tree generally node decision tree decision called leaf tree decision tree intuitive easy build fall short come accuracy
Model,kernel explain kernel trick,kernel computing dot vector possibly dimensional feature space kernel function sometimes called generalized dot kernel trick method linear classifier solve linear problem transforming linearly inseparable linearly separable one higher dimension
Model,beneficial perform dimensionality reduction fitting svm,greater observation performing dimensionality reduction generally improve svm
Methodology,overfitting,overfitting error fit resulting variance bias consequence overfit inaccurately predict point though accuracy
Methodology,boosting,boosting ensemble method improve reducing bias variance ultimately converting weak learner strong learner idea train weak learner sequentially iterate improve learning learner
Statistics,difference convex convex function mean function convex,convex function drawn point graph lie above graph minimum convex function drawn point graph intersect point graph characterized wavy function convex mean likelihood function minimum instead global minimum typically undesired machine learning model optimization perspective
EDA,outlier explain might screen outlier dataset explain inlier might screen dataset,outlier differs significantly observation depending cause outlier bad machine learning perspective worsen accuracy outlier caused measurement error important remove dataset couple way identify outlier z score standard deviation lie standard deviation calculate standard deviation multiply identify point outside range likewise calculate z score given equal outlier few contingency considered method normally distributed applicable set presence outlier throw z score interquartile range iqr iqr concept build boxplots identify outlier iqr equal difference rd quartile st quartile identify outlier le q irq greater q iqr come approximately standard deviation method dbscan clustering isolation forest robust random cut forest inlier observation lie rest dataset unusual error lie dataset typically harder identify outlier requires external identify identify inliers simply remove dataset
EDA,handle missing imputation technique recommend,several way handle missing delete row missing mean median mode imputation assigning unique predicting missing value algorithm support missing value random forest method delete row missing ensures bias variance added removed ultimately robust accurate recommended lot percentage missing value
Methodology,compiling uploaded month notice spike uploads particular spike picture uploads might cause test,potential reason spike uploads feature implemented involves uploading gained lot traction user example feature give ability create album similarly possible uploading intuitive improved month viral social movement involved uploading lasted eg movember something scalable possible spike due posting themselves costume halloween method testing depends cause spike conduct hypothesis testing determine inferred cause actual cause
Statistics,calculate needed sample,margin error formula determine desired sample
Methodology,running pollster polled hundred sixty claimed vote relax,assume opponent assume confidence interval give z score hat z give confidence interval therefore given confidence interval okay worst scenario tying relax otherwise relax until got claim
Methodology,given train having column million row classification problem manager asked reduce dimension computation reduced machine memory constraint practical assumption,lower ram close application machine browser memory put randomly sample mean create smaller let having variable row computation reduce dimensionality separate numerical categorical variable remove correlated variable numerical variable ll correlation categorical variable ll chi square test pca pick component explain maximum variance learning algorithm vowpal wabbit python possible option building linear stochastic gradient descent helpful apply understanding estimate predictor impact response variable intuitive approach failing identify useful predictor might result significant loss
Methodology,rotation necessary pca happen don rotate component,rotation orthogonal necessary maximizes difference variance captured component make component easier interpret forget motive doing pca aim fewer component explain maximum variance doing rotation relative component doesn change actual coordinate point don rotate component effect pca diminish ll component explain variance
Statistics,given missing value spread along standard deviation median percentage remain unaffected,question enough hint thinking spread across median let assume normal distribution normal distribution lie standard deviation mean mode median leaf unaffected therefore remain unaffected missing value
Statistics,given cancer detection ve build classification achieved accuracy shouldn happy performance,worked enough set deduce cancer detection imbalanced imbalanced accuracy measure performance given might predicting majority correctly interest minority actually got diagnosed cancer hence evaluate performance sensitivity true positive specificity true negative measure determine wise performance classifier minority performance poor undertake step undersampling oversampling smote balanced alter prediction threshold doing probability caliberation finding optimal threshold auc roc curve assign weight class minority class get larger weight anomaly detection
Model,naive bayes naive,naive bayes naive assumes equally important independent assumption rarely true scenario
Model,explain prior probability likelihood marginal likelihood context naive bayes algorithm,prior probability nothing proportion dependent binary variable closest guess further example dependent variable binary proportion spam spam hence estimate chance classified spam likelihood probability classifying given observation presence variable example probability word spam likelihood marginal likelihood probability word
Model,working manager asked build accuracy decision tree algorithm work fairly kind later tried regression got higher accuracy decision tree happen,known posse linearity hand decision tree algorithm known detect linear interaction reason decision tree failed robust prediction couldn linear relationship regression therefore learned linear regression robust prediction given satisfies linearity assumption
Methodology,assigned involves helping delivery problem delivery aren able deliver result customer unhappy keep happy delivering machine learning algorithm,might started hopping ml algorithm mind wait asked test machine learning fundamental machine learning problem route optimization problem machine learning problem consist thing exist pattern solve mathematically writing exponential equation always factor decide machine learning tool solve particular problem
Methodology,came suffering bias variance algorithm tackle,bias occurs predicted value near actual value word becomes flexible enough mimic distribution sound achievement forget flexible generalization capability mean tested unseen give disappointing situation bagging algorithm random forest tackle variance problem bagging algorithm divide subset repeated randomized sampling sample generate model single learning algorithm later prediction combined voting classification averaging regression combat variance regularization technique higher coefficient penalized hence lowering complexity variable importance chart variable algorithm having difficulty finding meaningful signal
EDA,given contains variable highly correlated manager asked run pca remove correlated variable,chance might tempted incorrect discarding correlated variable substantial effect pca presence correlated variable variance explained particular component get inflated example variable correlated run pca principal component exhibit twice variance exhibit uncorrelated variable adding correlated variable let pca put importance variable misleading
Methodology,spending several anxious build accuracy result build gbm model thinking boosting algorithm magic unfortunately neither model perform benchmark score finally decided combine model though ensembled model known accuracy unfortunate miss,ensemble learner idea combining weak learner create strong learner learner superior result combined model uncorrelated gbm model got accuracy improvement suggests model correlated problem correlated model model example classified chance done actual therefore ensemble learner built premise combining weak uncorrelated model obtain prediction
Model,knn kmeans clustering,don mislead name fundamental difference algorithm kmeans unsupervised nature knn supervised nature kmeans clustering algorithm knn classification regression algorithm kmeans algorithm partition cluster cluster formed homogeneous point cluster close algorithm try maintain enough separability cluster due unsupervised nature cluster label knn algorithm try classify unlabeled observation surrounding neighbor known lazy learner involves minimal hence doesn generalization unseen
Statistics,true positive recall write equation,true positive recall equal having formula tp tp fn
Model,built multiple regression isn wanted improvement remove intercept term becomes possible,possible understand significance intercept term regression intercept term show prediction independent variable mean prediction formula ymean predicted intercept term present evaluates wrt mean absence intercept term ymean evaluation denominator equation becomes smaller actual resulting higher
Model,analyzing manager informed regression suffering multicollinearity true losing build,multicollinearity create correlation matrix identify remove variable having correlation above deciding threshold subjective addition calculate vif variance inflation factor presence multicollinearity vif implies serious multicollinearity tolerance indicator multicollinearity removing correlated variable might lead loss retain variable penalized regression model ridge lasso regression random noise correlated variable variable become adding noise might affect prediction accuracy hence approach carefully
Model,ridge regression favorable lasso regression,conceptually lasso regression variable selection parameter shrinkage whereas ridge regression parameter shrinkage coefficient presence correlated variable ridge regression might preferred choice ridge regression work situation least square estimate higher variance therefore depends objective
Statistics,rise global average temperature led decrease pirate mean decrease pirate caused climate,reading question understood classic causation correlation conclude decrease pirate caused climate might factor lurking confounding variable influencing phenomenon therefore might correlation global average temperature pirate pirated died rise global average temperature
EDA,working important variable explain method,method variable selection remove correlated variable prior selecting important variable linear regression variable value forward selection backward selection stepwise selection random forest xgboost plot variable importance chart lasso regression measure gain accordingly
Statistics,difference covariance correlation,correlation standardized covariance covariance difficult example calculate covariance salary age ll covariance compared having unequal scale combat situation calculate correlation irrespective respective scale
Statistics,possible capture correlation continuous categorical variable,ancova analysis covariance technique capture association continuous categorical variable
Model,tree algorithm random forest gradient boosting algorithm gbm,fundamental difference random forest us bagging technique prediction gbm us boosting technique prediction bagging technique divided sample randomized sampling single learning algorithm build sample later resultant prediction combined voting averaging bagging done parallel boosting round prediction algorithm weighs misclassified prediction higher corrected succeeding round sequential giving higher weight misclassified prediction continue until stopping criterion reached random forest improves accuracy reducing variance mainly tree grown uncorrelated maximize decrease variance hand gbm improves accuracy reducing bias variance
Model,running binary classification tree algorithm easy tree splitting take tree decide variable split root node succeeding node,classification tree make decision gini node entropy simple word tree algorithm possible feature divide purest possible node
Model,ve built random forest tree got delighted getting error validation error haven trained perfectly,overfitted error mean classifier mimiced pattern extent unseen hence classifier run unseen sample couldn pattern returned prediction higher error random forest happens larger tree necessary hence avoid situation tune tree cross validation
Methodology,ve got having variable observation ols bad option technique,dimensional set classical regression technique assumption tend fail longer calculate unique least square coefficient estimate variance become infinite ols combat situation penalized regression method lasso lars ridge shrink coefficient reduce variance precisely ridge regression work situation least square estimate higher variance among method subset regression forward stepwise regression
Model,convex hull hint svm,linearly separable convex hull represents outer boundary point once convex hull created maximum margin hyperplane mmh perpendicular bisector convex hull mmh attempt create greatest separation group
EDA,encoding increasing dimensionality label encoding doesn,don baffled question simple question asking difference encoding dimensionality increased creates variable present categorical variable example let variable color variable level namely red blue green encoding color variable generate variable color red color blue color green containing label encoding level categorical variable get encoded variable created label encoding majorly binary variable
Model,cross validation technique fold loocv,neither problem fold troublesome might pattern resampling separate trend might validation past incorrect instead forward chaining strategy fold shown below fold test fold test fold test fold test fold test represents
EDA,given consisting variable having missing value let variable variable missing value higher deal,deal way assign unique missing value know missing value might decipher trend remove blatantly sensibly distribution target variable pattern ll keep missing value assign removing others
Methodology,bought bought recommendation seen amazon result algorithm,basic idea kind recommendation engine come collaborative filtering collaborative filtering algorithm considers behavior recommending exploit behavior user transaction rating selection purchase user behaviour preference recommend user known
Statistics,understand v ii error,error committed null hypothesis true reject known false positive ii error committed null hypothesis false accept known false negative context confusion matrix error occurs classify positive actually negative ii error occurs classify negative actually positive
Methodology,working classification problem validation purpose ve randomly sampled train validation confident incredibly unseen validation accuracy shocked getting poor test accuracy went wrong,classification problem always stratified sampling instead random sampling random sampling doesn take consideration proportion target class contrary stratified sampling help maintain distribution target variable resultant distributed sample
Model,asked evaluate regression adjusted tolerance criterion,tolerance vif indicator multicollinearity indicator percent variance predictor accounted predictor value tolerance desirable consider adjusted opposed evaluate fit increase irrespective improvement prediction accuracy variable adjusted increase additional variable improves accuracy otherwise stay difficult commit threshold adjusted varies set example gene mutation might result lower adjusted fairly prediction compared lower adjusted implies
Model,mean knn euclidean distance calculate distance nearest neighbor manhattan distance,don manhattan distance calculates distance horizontally vertically dimension restriction hand euclidean metric space calculate distance point present dimension euclidean distance viable option
Methodology,explain machine learning,simple baby walk fall unconsciously realize leg straight bend position fall feel pain cry stand again avoid pain try harder succeed seek door wall anything near help stand firm machine work develops intuition environment
Model,linear regression generally evaluated adjusted evaluate logistic regression,logistic regression predict probability auc roc curve along confusion matrix determine performance analogous metric adjusted logistic regression aic aic measure fit penalizes coefficient therefore always prefer minimum aic null deviance indicates response predicted nothing intercept lower residual deviance indicates response predicted adding independent variable lower
Methodology,considering machine learning algorithm given decide,choice machine learning algorithm solely depends given exhibit linearity linear regression algorithm given image audio neural build robust comprises linear interaction boosting bagging algorithm choice requirement build deployed ll regression decision tree easy interpret explain instead algorithm svm gbm etc short master algorithm situation scrupulous enough understand algorithm
EDA,suggest treating categorical variable continuous variable result predictive,prediction categorical variable considered continuous variable variable ordinal nature
Methodology,regularization becomes necessary machine learning,regularization becomes necessary begin ovefit underfit technique introduces term bringing objective function hence try push coefficient variable zero hence reduce term help reduce complexity become predicting generalizing
Model,ols linear regression maximum likelihood logistic regression explain statement,ols maximum likelihood method respective regression method approximate unknown parameter coefficient simple word ordinary least square ols method linear regression approximates parameter resulting minimum distance actual predicted value maximum likelihood help choosing value parameter maximizes likelihood parameter likely produce observed
Model,logistic regression done,logistic regression measure relationship dependent variable label predict independent variable estimating probability underlying logistic function sigmoid
Model,explain step making decision tree,entire input calculate entropy target variable predictor attribute calculate gain attribute gain sorting object choose attribute highest gain root node repeat procedure branch until decision node branch finalized
Model,build random forest,random forest built decision tree split package decision tree group random forest brings tree together step build random forest randomly among calculate node split split node daughter node split repeat step until leaf node finalized build forest repeating step four create tree
Methodology,avoid overfitting,overfitting refers amount ignores bigger picture method avoid overfitting keep simple fewer variable thereby removing noise cross validation technique fold cross validation regularization technique lasso penalize certain parameter likely cause overfitting
EDA,feature selection method variable,method feature selection filter wrapper method filter method involves linear discrimination analysis anova chi square analogy selecting bad bad answer limiting selecting cleaning coming wrapper method involves forward selection test feature keep adding until fit backward selection test removing work recursive feature elimination recursively look pair together wrapper method labor intensive computer needed lot analysis performed wrapper method
Methodology,dimensionality reduction benefit,dimensionality reduction refers converting vast dimension fewer dimension field convey similar concisely reduction help compressing reducing storage space reduces computation fewer dimension lead le computing remove redundant example storing unit meter inch
Methodology,maintain deployed,step maintain deployed monitor constant monitoring model needed determine performance accuracy something figure change affect thing need monitored ensure doing supposed evaluate evaluation metric calculated determine algorithm needed model compared determine performs rebuild performing built
Methodology,recommender,recommender predicts specific preference split area collaborative filtering example fm recommends track user similar interest play often commonly seen amazon making purchase customer notice accompanied recommendation user bought bought filtering example pandora us property song recommend similar property instead looking else listening
Model,mean,elbow method mean clustering idea elbow method run mean clustering cluster sum square w defined sum squared distance cluster centroid
Statistics,significance,typically indicates strong evidence against null hypothesis reject null hypothesis typically indicates weak evidence against null hypothesis accept null hypothesis cutoff considered marginal meaning either
Model,declared stationery,stationary variance mean constant
Statistics,write equation calculate precision recall,precision true positive true positive false positive recall true positive positive false negative
Methodology,bought bought recommendation seen amazon result algorithm,recommendation engine accomplished collaborative filtering collaborative filtering explains behavior user purchase rating selection etc engine make prediction might interest person preference user algorithm unknown
Methodology,predict probability death heart disease risk factor age gender blood cholesterol appropriate algorithm,choose correct option logistic regression linear regression mean clustering apriori algorithm appropriate algorithm logistic regression
Methodology,organization visitor randomly receive coupon possible visitor receive coupon asked determine offering coupon visitor impact purchase decision analysis method,anova mean clustering association rule student test answer anova
Statistics,feature vector,feature vector dimensional vector numerical represent object machine learning feature vector represent numeric symbolic characteristic called object mathematical easy analyze
Model,step making decision tree,entire input split maximizes separation class split test divide set apply split input divide step apply step divided stop meet stopping criterion step called pruning clean tree went far doing split
Statistics,root cause analysis,root cause analysis initially developed analyze industrial accident widely area problem solving technique isolating root cause fault problem factor called root cause deduction problem fault sequence averts final undesirable event recurring
Model,logistic regression,logistic regression known logit technique forecast binary outcome linear combination predictor variable
Methodology,explain cross validation,cross validation validation technique evaluating outcome statistical analysis generalize independent mainly background objective forecast want estimate accurately accomplish practice goal cross validation term test phase validation limit problem overfitting gain insight generalize independent
Methodology,collaborative filtering,recommender filtering pattern collaborating perspective numerous source several agent
Methodology,gradient descent method always converge similar point,case reach minimum optimum reach global optimum governed starting
Methodology,goal testing,statistical hypothesis testing randomized experiment variable objective testing detect change maximize increase outcome strategy
Model,drawback linear,assumption linearity error count outcome binary outcome overfitting problem solve
Statistics,eigenvalue eigenvector,eigenvalue direction along particular linear transformation act flipping compressing stretching eigenvectors understanding linear transformation analysis usually calculate eigenvectors correlation covariance matrix
Statistics,difference estimate confidence interval,estimation give particular estimate population parameter method moment maximum likelihood estimator method derive estimator population parameter confidence interval give range value likely contain population parameter confidence interval generally preferred tell likely interval contain population parameter likeliness probability called confidence confidence coefficient represented alpha alpha significance
Statistics,understand statistical sensitivity calculate,sensitivity commonly validate accuracy classifier logistic svm random forest etc sensitivity nothing predicted true true true predicted true
Statistics,explain roc curve work,roc curve graphical representation contrast true positive rate false positive rate various threshold often proxy trade sensitivity true positive false positive
Methodology,tf idf vectorization,tf idf short term frequency inverse document frequency numerical statistic intended reflect important word document collection corpus often weighting factor retrieval mining tf idf increase proportionally word appears document offset frequency word corpus help adjust fact word appear frequently
Model,generally softmax linearity function operation,take vector number return probability distribution clear output probability distribution element negative sum component
Model,explain svm algorithm detail,svm stand vector machine supervised machine learning algorithm regression classification svm try plot dimensional space feature particular coordinate svm us hyperplanes separate class kernel function
Model,kernel svm,four type kernel svm linear kernel polynomial kernel radial basis kernel sigmoid kernel
Methodology,difference regression classification ml technique,regression classification machine learning technique come supervised machine learning algorithm supervised machine learning algorithm train labelled explicitly correct label algorithm try pattern input output label discrete value classification problem etc label continuous value regression problem etc
EDA,various step involved analytics,understand problem explore become familiar prepare modelling detecting outlier treating missing value transforming variable etc preparation running analyze result tweak approach iterative step until possible outcome achieved validate implementing track result analyze performance period
Model,mean deep learning,deep learning nothing paradigm machine learning shown incredible promise recent fact deep learning show analogy functioning brain
Methodology,difference machine learning deep learning,machine learning field give computer ability explicitly programmed machine learning categorised supervised machine learning unsupervised machine learning reinforcement learning deep learning subfield machine learning concerned algorithm inspired structure function brain called artificial neural network
Methodology,opinion reason popularity deep learning recent,although deep learning major breakthrough technique came recent reason increase amount generated various source growth hardware run model gpus multiple faster build bigger deeper deep learning model comparatively le previously
Statistics,function,referred loss error function measure evaluate performance compute error output layer backpropagation push error backwards neural function
Statistics,hyperparameters,neural network usually working hyperparameters once formatted correctly hyperparameter parameter whose learning begin determines trained structure hidden unit learning epoch etc
Model,layer cnn,convolutional layer layer performs convolutional operation creating several smaller picture relu layer brings linearity convert negative pixel zero output rectified feature pooling layer pooling sampling operation reduces dimensionality feature fully connected layer layer recognizes classifies object
Model,pooling cnn,pooling reduce spatial dimension cnn performs sampling operation reduce dimensionality creates pooled feature sliding filter matrix input matrix
Model,recurrent neural network rnns,rnns artificial neural network designed recognise pattern sequence agency etc understand recurrent net understand basic feedforward net network rnn feed forward named channel mathematical oration performed node feed straight never touching node twice cycle loop latter called recurrent recurrent network hand input input example perceived previously decision recurrent neural reached affect decision reach moment later recurrent network source input present recent past combine determine respond error generate via backpropagation adjust weight until error lower remember purpose recurrent net accurately classify sequential input rely backpropagation error gradient descent
Model,lstm,short term memory lstm kind recurrent neural capable learning term dependency remembering period default behaviour step lstm step decides forget remember step selectively update cell value step decides make output
Model,multi layer perceptron mlp,neural network mlps input layer hidden layer output layer structure single layer perceptron hidden layer single layer perceptron classify linear separable class binary output mlp classify nonlinear class except input layer node layer us nonlinear activation function mean input layer coming activation function upon node weight added together producing output mlp us supervised learning method called backpropagation backpropagation neural calculates error function propagates error backward came adjusts weight train accurately
Model,explain gradient descent,understand gradient descent let understand gradient gradient measure output function change input bit simply measure weight regard error gradient slope function gradient descent thought climbing bottom valley instead climbing hill minimization algorithm minimizes given function activation function
Model,exploding gradient,rnn exponentially growing error gradient accumulate result update neural weight known exploding gradient extreme value weight become overflow result nan value effect unstable unable
Model,vanishing gradient,rnn slope become either make difficult slope problem known vanishing gradient lead poor performance accuracy
Model,propagation explain working,backpropagation algorithm multilayer neural method move error weight inside thus allowing efficient computation gradient step forward propagation derivative computed output target propagate computing derivative error wrt output activation previously calculated derivative output update weight
Model,variant propagation,stochastic gradient descent single example calculation gradient update parameter batch gradient descent calculate gradient whole dataset perform update iteration mini batch gradient descent popular optimization algorithm variant stochastic gradient descent instead single example mini batch sample
Model,auto encoder,auto encoders simple learning network aim transform input output minimum possible error mean output close input possible couple layer input output size layer smaller input layer auto encoder receives unlabelled input encoded reconstruct input
Model,dropout batch normalization,dropout technique dropping hidden visible unit randomly prevent overfitting typically dropping cent node double iteration needed converge batch normalization technique improve performance stability neural network normalizing input layer mean output activation zero standard deviation
Model,mean tensor tensorflow,tensor mathematical object represented array higher dimension array dimension rank fed input neural called tensor
Statistics,central limit theorem important,suppose interested estimating average height among collecting person impossible obtain height measurement everyone population sample question becomes average height entire population given single sample central limit theorem address question exactly
Model,assumption linear regression,four major assumption linear relationship dependent variable regressors meaning creating actually fit error residual normally distributed independent minimal multicollinearity explanatory variable homoscedasticity mean variance regression value predictor variable
EDA,explain rule tell importance validation,usually tend split test split once ratio create validation
Model,batch normalization explain,idea standardize sending layer approach help reduce impact layer keeping mean variance constant make layer independent achieve rapid convergence example normalize help accelerate learning cycle
EDA,choosing algorithm solve problem,develop problem statement problem step essential ll ensure fully understand problem input output problem solve problem statement simple single sentence example let consider enterprise spam requires algorithm identify problem statement fake spam scenario identification whether fake spam output once defined problem statement identify appropriate algorithm classification algorithm clustering algorithm regression algorithm recommendation algorithm algorithm depend specific problem trying solve scenario move forward clustering algorithm choose mean algorithm achieve goal filtering spam example aren always necessary answering artificial intelligence sometimes easier across
Methodology,difference inductive deductive abductive learning,inductive learning describes smart algorithm instance draw conclusion statistical ml nearest neighbor vector machine example inductive learning literal inductive learning arithmetic literal equality inequality predicate deductive learning smart algorithm draw conclusion truth generating structure major premise minor premise conclusion improve decision scenario ml algorithm engages deductive reasoning decision tree abductive learning dl technique conclusion various instance approach inductive reasoning applied causal relationship deep neural network
Methodology,step evaluate effectiveness ml,split test set option cross validation technique further segment composite test set implement choice selection performance metric confusion matrix accuracy precision recall sensitivity specificity score measure accuracy confusion matrix score ll critical demonstrate understand nuance measured choosing performance measure match problem
Model,bayes theorem useful machine learning context,bayes theorem give posterior probability event given known prior knowledge mathematically expressed true positive condition sample divided sum false positive population true positive condition chance actually having flu flu test flu test false overall population chance having flu actually chance having flu having positive test bayes theorem say say true positive condition sample true positive condition sample false positive population chance getting flu bayes theorem basis behind branch machine learning notably includes naive bayes classifier something important consider faced machine learning interview
Model,naive bayes naive,despite practical application especially mining naive bayes considered naive make assumption virtually impossible conditional probability calculated pure individual probability component implies absolute independence condition probably never met quora commenter put whimsically naive bayes classifier figured liked pickle ice cream probably naively recommend pickle ice cream
Model,explain difference regularization,regularization tends spread error among binary sparse variable either assigned weighting corresponds setting laplacean prior corresponds gaussian prior
Statistics,fourier transform,fourier transform generic method decompose generic function superposition symmetric function intuitive tutorial put given smoothie recipe fourier transform find cycle speed amplitude phase match signal fourier transform convert signal frequency domain common extract audio signal sensor
Methodology,deep learning contrast machine learning algorithm,deep learning subset machine learning concerned neural network backpropagation certain principle neuroscience accurately set unlabelled semi structured sense deep learning represents unsupervised learning algorithm learns representation neural net
Methodology,difference generative discriminative,generative discriminative simply distinction discriminative model generally outperform generative model classification task
Statistics,score,score measure performance weighted average precision recall tending tending worst classification test true negative don matter
Methodology,classification regression,classification produce discrete value dataset strict regression give continuous allow distinguish difference individual point classification regression wanted reflect belongingness point dataset certain explicit
Methodology,kernel trick useful,kernel trick involves kernel function enable higher dimension space explicitly calculating coordinate point dimension instead kernel function compute inner image pair feature space allows useful attribute calculating coordinate higher dimension computationally cheaper explicit calculation coordinate algorithm expressed inner kernel trick enables effectively run algorithm dimensional space lower dimensional
Methodology,experience spark machine learning,ll familiar meaning company ll spark tool demand able handle immense datasets speed honest don experience demanded description pop ll invest familiarizing yourself
Methodology,build pipeline,pipeline bread butter machine learning engineer model way automate scale sure familiar build pipeline apache airflow platform host model pipeline google cloud aws azure explain step functioning pipeline talk actual experience building scaling production
EDA,always,statistically depends example biased getting won depends suffers bias getting won improve test beyond etc practically tradeoff having additional storage computational memory requires hence always having
EDA,sure don analyze something end meaningless,proper exploratory analysis analysis task exploratory phase graphing thing testing thing set summarizing simple statistic getting rough idea hypothesis might pursue further exploitatory phase deeply hypothesis exploratory phase generate lot possible hypothesis exploitatory phase let understand few balance ll prevent yourself wasting thing meaningless although
EDA,determine important,run though gradient boosting machine random forest generate plot relative importance gain feature ensemble variable added forward variable selection
Methodology,way robust outlier,regularization reduce variance increase bias change algorithm tree method instead regression method resistant outlier statistical test parametric test instead parametric one robust error metric mae huber loss instead mse change winsorizing transforming log remove certain anomaly worth predicting
Methodology,difference expect minimizes squared error versus minimizes absolute error case error metric appropriate,mse strict having outlier mae robust sense harder fit numerically optimized le variability computationally easy fit mae mse mse easier compute gradient mae linear programming needed compute gradient mae robust outlier consequence error mse mse corresponds maximizing likelihood gaussian random variable
Methodology,error metric evaluate binary classifier class imbalanced group,accuracy proportion instance predict correctly pro intuitive easy explain con work poorly label imbalanced signal weak auroc plot fpr axis tpr axis threshold given random positive instance random negative instance auc probability identify pro work testing ability distinguishing class con interpret prediction probability auc determined ranking explain uncertainty logloss deviance pro error metric probability con sensitive false positive negative group binary classification logloss metric auc applicable binary
Methodology,various way predict binary response variable tell appropriate difference svm logistic regression naive bayes decision tree etc,thing linearly seperable independent likely overfit speed performance memory usage logistic regression roughly linear problem roughly linearly separable robust noise regularization selection avoid overfitting output come probability efficient computation distributed baseline algorithm hardly handle categorical svm nonlinear kernel deal problem linearly separable slow train scale application efficient naive bayes computationally efficient alleviating curse dimensionality work surprisingly case condition doesn hold word frequency independence assumption seen reasonable algorithm categorization conditional independence feature met tree ensemble deal categorical parametric worry outlier gbt parameter harder tune rf work usually performs worse gbt deep learning work classification task squeeze something problem
Methodology,given tweet retweets predict retweets given tweet observing worth,build seven cycle ask someone build regression function estimate retweets function determine regression function built cluster trend retweets regression function retweets predict seventh
Methodology,test assignment various bucket truly random,plot distribution multiple sure shape rigorously conduct permutation test distribution manova mean
Methodology,might benefit running test bucket exposed exact,verify sampling algorithm random
Methodology,hazard letting user sneak peek bucket test,might act suppose seen bucket essentially adding additional variable whether peeked bucket random across group
Methodology,run test variant,treatment sample enough way attempt correct changing confidence bonferroni correction doing wide test dive individual metric fisher protected lsd
Methodology,run test observation extremely skewed,lower variability modifying kpi cap value percentile metric log transform
Statistics,difference error,defined null hypothesis le fraction parameter value extreme observed parameter probability null hypothesis wrong error rejecting ho ho true error rejecting ho ha true
Statistics,airbnb test hypothesis greater photograph increase chance buyer selects test hypothesis,randomly selected listing hide random picture booking group
Statistics,experiment determine impact latency engagement,quantify impact performance isolate factor slowdown experiment delay test
Statistics,maximum likelihood estimation doesn exist,method parameter optimization fitting choose parameter maximize likelihood function likely outcome happen given maximum likelihood estimation mle method estimating parameter statistical given observation finding parameter value maximize likelihood making observation given parameter mle seen maximum posteriori estimation assumes uniform prior distribution parameter variant ignores prior therefore unregularized gaussian mixture parametric model doesn exist
Statistics,confidence interval interpret,example confidence interval interval constructed sample sampled constructed interval true mean confidence interval constructed given confidence infinite independent experiment proportion interval contain true parameter match confidence
Statistics,selection bias,selection bias kind error occurs researcher decides studied usually associated selection participant isn random sometimes referred selection effect distortion statistical analysis resulting method collecting sample selection bias taken conclusion study accurate type selection bias sampling bias systematic error due random sample population causing population le likely included others resulting biased sample interval bias trial terminated early extreme often ethical reason extreme likely reached variable largest variance variable similar mean specific subset chosen conclusion rejection bad arbitrary ground instead according previously stated generally agreed criterion attrition attrition bias kind selection bias caused attrition loss participant discounting trial subject test run completion
Methodology,regression model solve regression problem,regression supervised ml regression model investigate relationship dependent target independent variable predictor common regression model linear regression establishes linear relationship target predictor predicts numeric shape straight polynomial regression regression equation independent variable curve fit point ridge regression help predictor highly correlated multicollinearity problem penalizes square regression coefficient doesn allow coefficient reach zero us regularization lasso regression penalizes absolute value regression coefficient allows coefficient reach absolute zero thereby allowing feature selection
Model,assumption linear regression,several assumption linear regression violated prediction interpretation worthless misleading linear relationship target variable additivity mean effect change target variable depend value example predicting revenue sold sold sell revenue increase independent sold customer stop buying additivity assumption violated correlated collinearity difficult separate individual effect collinear target variable error independently identically normally distributed yi errori correlation error consecutive error constant variance error homoscedasticity example seasonal pattern increase error season higher activity error normaly distributed otherwise influence target variable others error distribution significantly normal confidence interval wide narrow
Statistics,normal distribution,normal distribution derives importance central limit theorem draw enough sample mean follow normal distribution regardless initial distribution sample distribution mean sample normal important sample independent powerful help study process whose population distribution unknown
Statistics,variable follows normal distribution,plot histogram sampled fit bell shaped normal curve histogram hypothesis underlying random variable follows normal distribution rejected skewness kurtosis sampled skewness kurtosis typical normal distribution farther away value normal distribution kolmogorov smirnov shapiro wilk test normality skewness kurtosis simultaneously quantile quantile plot scatterplot created plotting set quantiles against normal q q plot point roughly straight
Methodology,build predicting distributed normally pre processing,normal specially datasets uncleaned datasets always certain skewness go prediction house thing consideration depends factor chance presence skewed value outlier talk pre processing probably remove outlier distribution near normal
Model,method solving linear regression,solve linear regression coefficient minimize sum squared error matrix algebra method let matrix vector value predict matrix algebra minimization problem solution solving requires inverse consuming impossible luckily method singular decomposition svd qr decomposition reliably calculate called pseudo inverse actually needing inverse popular python ml sklearn us svd solve least square
Model,gradient descent,gradient descent algorithm us calculus concept gradient try reach global minimum work taking negative gradient given function updating repeatedly calculated negative gradient until algorithm reach global minimum cause future iteration algorithm value equal close widely machine learning application
Statistics,normal equation,normal equation equation obtained setting equal zero partial derivative sum squared error least square normal equation allow estimate parameter multiple linear regression
Methodology,sgd stochastic gradient descent difference usual gradient descent,gradient descent gd stochastic gradient descent sgd update parameter iterative manner minimize error function gd run sample single update parameter particular iteration sgd hand subset sample update parameter particular iteration subset called minibatch stochastic gradient descent
Methodology,metric evaluating regression model,mean squared error mse root mean squared error rmse mean absolute error mae coefficient determination adjusted
Statistics,mse rmse,mse stand mean square error rmse stand root mean square error metric evaluate model
Statistics,bias variance trade,bias error introduced approximating true underlying function quite complex simpler variance sensitivity change dataset bias variance trade relationship expected test error variance bias contribute test error ideally possible expectedtesterror variance bias irreducibleerror complexity increase bias decrease variance increase lead overfitting vice versa simplification help decrease variance increase bias lead underfitting
Methodology,fold cross validation,fold cross validation method cross validation hyperparameter dataset divided part st validation remaining nd validation remaining part validation once remaining part taken together
Methodology,choose fold cross validation favorite,thing consider deciding model validation model le least model give le biased decision metric hand dataset least entire least ratio validation maintained tend datasets one
Model,logistic regression linear,logistic regression considered generalized linear outcome always depends sum input parameter word output depend quotient etc parameter
Model,logistic regression,logistic regression machine learning algorithm binary classification logistic regression variable take value true false spam spam churn churn variable binary dichotomous
Model,sigmoid,sigmoid function activation function specifically defined squashing function squashing function limit output range making function useful prediction probability sigmod
Statistics,precision recall trade,tradeoff mean increasing parameter lead decreasing precision recall tradeoff occur due increasing parameter precision recall keeping ideal scenario perfectly separable precision recall maximum practical situation noise dataset dataset perfectly separable might point positive closer negative vice versa case shifting decision boundary either increase precision recall increasing parameter lead decreasing
Statistics,roc curve,roc stand receiver operating characteristic diagrammatic representation show contrast true positive v false positive predict probability binary outcome
Statistics,auc au roc,auc stand roc curve roc probability curve auc represents degree measure separability capable distinguishing class higher
Statistics,interpret au roc score,auc score roc curve excellent auc near mean measure separability poor auc near mean worst measure separability auc score mean separation capacity whatsoever
Statistics,pr precision recall curve,precision recall curve pr curve plot precision axis recall axis probability threshold precision recall curve pr curve recommended highly skewed domain roc curve excessively optimistic performance
Statistics,pr curve useful metric,precision recall auc roc auc summarizes curve range threshold value single score curve represents recall precision precision relates false positive recall relates false negative
Statistics,case au pr au roc,au roc look true positive tpr false positive fpr au pr look positive predictive ppv true positive tpr typically true negative meaningful problem positive au pr typically useful otherwise equally positive negative dataset quite balanced au roc idea
EDA,categorical variable,categorical variable encoded train machine learning various encoding technique encoding label encoding ordinal encoding target encoding
Model,kind regularization technique applicable linear model,aic bic ridge regression lasso elastic net basis pursuit denoising rudin osher fatemi potts rlad dantzig selector slope
Model,effect regularization weight linear,regularization penalizes larger weight severely due squared penalty term encourages weight value decay toward zero
Model,regularization look linear,regularization add penalty term function equal sum module model coefficient multiplied lambda hyperparameter
Model,difference regularization,penalty regularization us sum absolute value weight regularization us sum weight squared feature selection performs feature selection reducing coefficient predictor computational efficiency analytical solution multicollinearity address multicollinearity constraining coefficient norm
Model,regularization component linear,elastic net regularization combine regularization
Model,interpretation bias term linear model,bias simply difference predicted actual true interpreted distance average prediction true true minus mean prediction dont confused accuracy bias
Model,interpret weight linear model,normalizing weight variable increase corresponding predictor unit coefficient represents average output change interpretation work logistic regression increase corresponding predictor unit weight represents log odds variable normalized interpret weight linear model importance variable predicted result
Model,weight variable higher variable important,predictor variable normalized normalization weight represents output unit predictor predictor huge range scale predict output range example nation gdp predict maternal mortality rate coefficient necessarily mean predictor variable important compared others
Model,perform feature normalization linear model okay,feature normalization necessary regularization idea method penalize relatively equally done effectively feature scaled differently linear regression regularization technique feature normalization regularization analytical solution stable add regularization matrix feature matrix inverting
EDA,feature selection technique,feature selection principal component analysis neighborhood component analysis relieff algorithm
EDA,regularization feature selection,nature regularization lead sparse coefficient feature selection done keeping zero coefficient
Model,benefit single decision tree compared complex model,easy implement fast fast inference explainability
Model,important decision tree,often split minimizes sum node impurity impurity criterion parameter decision tree popular method measure impurity gini impurity entropy describing gain
Model,randomization random forest,random forest extention bagging algorithm take random sample dataset replacement train several model average prediction addition split tree considered random forest take random sample replacement us subset candidate split example sqrt decision tree random sample dataset reduces variance sampling split decision tree decorrelates tree
Model,depth tree random forest,greater depth greater amount extracted tree limit algorithm defensive against overfitting complex noise present result overfit noise hence hard thumb rule deciding depth literature suggests few tip tuning depth tree prevent overfitting limit maximum depth tree limit test node limit minimum object node split split node least resulting subsample size below given threshold stop developing node sufficiently improve fit
Model,tree random forest,tree random forest worked estimator random forest reduces overfitting increasing tree fixed thumb rule decide tree random forest rather fine tuned typically starting taking square present followed tuning until optimal
EDA,happens correlated,random forest random forest sample build tree contained correlated twice likely picked contained adding correlated mean linearly contains thus reduce robustness train might pick feature explain variance reduce entropy etc
Model,gradient boosting tree,gradient boosting machine learning technique regression classification problem produce prediction ensemble weak prediction model typically decision tree
Model,difference random forest gradient boosting,random forest build tree independently gradient boosting build tree random forest combine averaging majority rule gradient boosting combine along
Model,possible parallelize gradient boosting,framework option faster gpus speed making highly parallelizable example xgboost tree method gpu hist option make faster gpus
Model,approach tuning parameter xgboost lightgbm,depending upon dataset parameter tuning done manually hyperparameter optimization framework optuna hyperopt manual parameter tuning aware max depth min sample leaf min sample split overfit try predict generalized characteristic basically keeping variance bias
Model,tree gradient boosting,implementation gradient boosting configured default relatively tree hundred thousand scikit perform grid estimator parameter
Methodology,hyper parameter tuning strategy difference grid parameter tuning strategy random,several strategy hyper tuning argue popular nowadays grid exhaustive approach hyper parameter need manually give value algorithm try value selected grid evaluates algorithm combination hyper parameter return combination give optimal result lowest mae grid evaluates given algorithm combination easy quite computationally expensive lead sub optimal specifically need specify specific value hyper parameter prone error requires domain knowledge random similar grid differs sense rather specifying value try hyper parameter upper lower bound value hyper parameter given instead uniform probability random value bound chosen similarly combination returned although seems le intuitive domain knowledge necessary theoretically parameter space explored completely framework bayesian optimization thought statistical optimization commonly neural network specifically evaluation neural computationally costly numerous paper method heavily outperforms grid random currently google cloud platform aws depth explanation requires heavy background bayesian statistic gaussian process maybe theory simple explanation simpler faster acquisition function intelligently chooses surrogate function probability improvement gp ucb hyper parameter value try computationally expensive original algorithm result initial combination value expensive original function acquisition function take result expensive original algorithm us prior knowledge again come hyper parameter choose iteration continues either specified iteration specified amount similarly combination hyper parameter performs expensive original algorithm chosen
Model,kind problem neural net solve,neural net solving linear problem example problem relatively easy human experience intuition understanding etc difficult traditional regression model speech recognition handwriting recognition identification etc
Model,usual fully connected feed forward neural,usual fully connected feed forward neuron receives input element layer thus receptive field neuron entire layer usually represent feature vector input classification problem expensive train computation involved
Model,activation function,idea neural network complex nonlinear function activation function layer neural stacking multiple linear layer lead learning linear function nonlinearity come activation function reason activation function
Model,problem sigmoid activation function,derivative sigmoid function positive negative number almost zero come problem vanishing gradient backpropagation net drastically slow possible solve problem relu activation function
Model,relu sigmoid tanh,relu abbreviation rectified linear unit activation function negative value positive value relu simple activation function make fast compute sigmoid tanh activation function saturate higher value relu potentially infinite activation address problem vanishing gradient
Model,initialize weight neural,proper initialization weight matrix neural necessary simply way initializtions initializing weight zero setting weight zero make linear important setting bias create trouble zero weight breaking symmetry bias value neuron initializing weight randomly assigning random value weight assignment weight initialized value term np dot becomes significantly higher activation function sigmoid applied function map near slope gradient change slowly learning take lot weight initialized value get mapped above problem often referred vanishing gradient
Model,weight neural,weight neural zero output connection mean gradient backpropagated connection layer mean connection weight thing never converges
Model,regularization technique neural net,regularization defined sum absolute value individual parameter penalty cause subset weight become zero suggesting corresponding safely discarded regularization defined sum square individual parameter often supported regularization hyperparameter alpha weight decay augmentation requires fake created drop effective regularization technique newral net few randome node layer deactivated forward pas allows algorithm train node iteration
Model,dropout useful,dropout technique step turn neuron certain probability iteration train neuron force rely subset neuron feature representation lead regularizing effect controlled hyperparameter
Model,backpropagation,backpropagation algorithm look minimum error function weight space technique called delta rule gradient descent weight minimize error function considered solution learning problem backpropogation calculate error far output actual output minimum error whether error minimized update parameter error huge update parameter weight bias again error repeat until error becomes minimum ready prediction once error becomes minimum feed input produce output
Model,optimization technique neural net,gradient descent stochastic gradient descent mini batch gradient descent among gradient descent nesterov accelerated gradient momentum adagrad adadelta adam le efficient
Model,sgd stochastic gradient descent neural net,sgd approximates expectation few randomly selected sample instead comparison batch gradient descent efficiently approximate expectation set sgd neural network reduces lot considering converge later random sampling add noise gradient descent
Model,learning,learning important hyperparameter control quickly adapted problem seen step width parameter update far weight moved direction minimum optimization problem
Model,learning,straightforward finding optimum learning involves lot hit trial usually starting value starting setting learning further tweaking doesn overshoot converge slowly
Model,adam difference adam sgd,adam adaptive moment estimation optimization technique neural network average optimizer work momentum intuition behind adam don roll fast jump minimum decrease velocity bit careful adam tends converge faster sgd often converges optimal solution sgd variance disadvantage get rectified adam advantage adam
Model,adam sgd,adam tends converge faster sgd often converges optimal solution
Model,decide stop neural net,simply stop validation error minimum
Model,checkpointing,saving weight learned mid running process known checkpointing resume certain checkpoint
Model,convolutional layer,idea convolutional layer assumption needed making decision often spatially close thus take weighted sum nearby input assumes network kernel reused node hence weight drastically reduced counteract feature learnt layer multiple kernel applied input creates parallel channel output consecutive layer stacked allow
Model,actually convolution fully connected layer,fully connected layer need weight inter layer connection mean weight need computed quickly balloon layer node layer increased
Model,pooling cnn,pooling technique downsample feature allows layer receive relatively undistorted version input line layer deeper abstract texture
Model,max pooling pooling technique,max pooling technique maximum receptive field passed feature commonly receptive field stride mean feature downsampled receptive field larger rarely employed lost pooling technique average pooling output average receptive field min pooling output minimum receptive field global pooling receptive field equal input mean output equal scalar reduce dimensionality feature
Model,cnns resistant rotation happens prediction cnn rotated,cnns resistant rotation model resistant augmenting datasets rotation raw prediction cnn rotated augment dataset accordingly
Model,augmentation,augmentation artifical expanding existing datasets performing transformation color shift thing help diversifying increasing scarcity train
Model,kind augmentation,kind augmentation according working geometric numerical transformation pca cropping padding shifting noise injection etc
Model,choose augmentation,augmentation depend output class eg mostly properly illuminated image dataset predict poorly illuminated image apply channel shifting resultant image dataset
Model,kind cnn architecture classification,classification inception xception densenet alexnet vgg resnet squeezenet efficientnet mobilenet designed smaller parameter helpful edge ai
Model,transfer learning,given domain learning task target domain learning task transfer learning aim improve learning target predictive function knowledge word transfer learning enables reuse knowledge coming domain learning task context cnns network pre trained popular datasets imagenet weight layer represent combine layer learns feature representation given class popular strategy either freeze layer feature representation completely give smaller learning
Methodology,bag word classification,bag word representation describes occurrence word document structure word considered classification histogram word consider word count feature
Methodology,advantage disadvantage bag word,advantage simple understand implement disadvantage vocabulary requires careful specifically manage impact sparsity document representation sparse representation harder computational reason space complexity reason discarding word ignores context turn meaning word document context meaning offer lot modeled tell difference word differently arranged interesting v interesting synonym bike v bike
Methodology,gram,function tokenize consecutive sequence word called gram co occurring word often word followed word given sentence
Methodology,tf idf useful classification,term frequency tf scoring frequency word document inverse document frequency idf scoring rare word across document scenario highly recurring word contain informational domain specific word example word frequent across document therefore le weighted tf idf score highlight word distinct contain useful given document
Model,prefer gradient boosting tree logistic regression doing classification bag word,usually logistic regression bag word creates matrix column huge column logistic regression usually faster gradient boosting tree
Methodology,sentence multiple word combine multiple word embeddings,approach ranked simple complex average word weighted average word weighting done inverse document frequency idf tf idf ml lstm transformer
Model,mean work,partition point subset compute seed point centroid cluster partitioning assign cluster nearest seed step stop assignment
Model,mean,domain knowledge expert know elbow method compute cluster value calculate cluster sum square plot sum according cluster band cluster average silhouette method compute cluster value calculate average silhouette observation plot silhouette according cluster maximum cluster
Methodology,clustering algorithm,medoids take central instead mean cluster make robust noise agglomerative hierarchical clustering ahc hierarchical cluster combining nearest cluster starting cluster divisive analysis clustering diana hierarchical clustering starting cluster containing point splitting cluster until describes cluster density spatial clustering application noise dbscan cluster defined maximum density connected point
Methodology,dbscan work,input parameter epsilon neighborhood radius minpts minimum point epsilon neighborhood cluster defined maximum density connected point point density connected epsilon minpts density reachable epsilon minpts density reachable epsilon minpts chain point directly density reachable directly density reachable neighborhood dist epsilon
Methodology,choose mean dbscan,dbscan robust noise dbscan amount cluster difficult guess mean lower complexity faster especially larger amount point
EDA,curse dimensionality,dimension relatively tightly packed adding dimension stretch point across dimension pushing further apart additional dimension spread further making dimensional extremely sparse difficult machine learning sparse space
Methodology,dimensionality reduction technique,singular decomposition svd principal component analysis pca linear discriminant analysis lda distributed stochastic neighbor embedding sne autoencoders fourier wavelet transforms
Methodology,singular decomposition typically machine learning,singular decomposition svd matrix decomposition method factor matrix matrix singular value diagonal matrix singular value machine learning principal component analysis pca typically svd singular value correspond eigenvectors value diagonal matrix square eigenvalue statistically descriptive having calculated eigenvectors eigenvalue kaiser guttman criterion scree plot proportion explained variance determine principal component final dimensionality useful dimensionality reduction
Statistics,precision recall,precision recall evaluation metric ranking algorithm precision show share relevant ranking algorithm recall indicates share relevant returned correct answer given query example query relevant algorithm return relevant precision num relevant result recall num relevant result num relevant
Methodology,baseline building recommender,recommer give relevant personalized recommend know find easily diverse suggestion explore
Methodology,collaborative filtering,collaborative filtering prominent approach generate recommendation us wisdom crowd give recommendation experience others recommendation calculated average experience give score indicates calculate experience user ui sum vi similar experience higher weight introduce similarity user multiplier user individual average larger normalization technique centering z score normalization remove user bias collaborative filtering matrix input improves sparse cold start below usually tends overfit
Methodology,incorporate implicit feedback click etc recommender,comparison explicit feedback implicit feedback datasets lack negative example example explicit feedback positive negative implicit feedback purchase click popular approach solve problem named weighted alternating least square wals hu koren volinsky collaborative filtering implicit feedback datasets mining icdm eighth ieee conference pp ieee instead modeling matrix directly number amount click describe strength observation action try latent factor predict expected preference
Methodology,cold problem,collaborative filterung incorporates crowd knowledge give recommendation certain recommend calculate score recommendation user certain distinguish way cold problem rated yet give recommendation calculate similarity
Methodology,possible approach solving cold problem,filtering incorporates calculate similarity recommend similarity liked already dependant rating user given anymore solve cold problem demographic filtering incorporates profile calculate similarity solves cold problem user
Model,usual regression problem,principle behind causal forecasting predicted dependant input causal factor forecasting predicted expected follow certain pattern
Model,model solving problem,simple exponential smoothing approximate exponentional function trend corrected exponential smoothing holt method exponential smoothing model trend trend seasonality corrected exponential smoothing holt winter method exponential smoothing model trend seasonality decomposition decomposed four component trend seasonal variation cycling varation irregular component autoregressive model similar multiple linear regression except dependent variable depends value rather independent variable deep learning approach rnn lstm etc
Methodology,trend remove,explicitly trend seasonality approach holt method holt winter method explicitly trend reach stationarity approach require stationarity stationarity interpretation analysis problematic manuca radu savit robert stationarity nonstationarity analysis physica nonlinear phenomenon
Methodology,variable measured predict approach,correlation observation measure correlation called autocorrelation autoregressive model multiple regression model lag original treated multiple independent variable
Methodology,variable predict approach,given assumption give meaningful causation causal forecasting approach linear regression multiple nonlinear regression might useful lot explanability priority consider deep learning approach
Model,problem tree solving problem,random forest model able extrapolate understand increasing decreasing trend average point validation value greater point
Methodology,common algorithm supervised learning unsupervised learning,supervised learning algorithm linear regression logistic regression decision tree random forest naive bayes neural network example unsupervised algorithm clustering mean visualization dimensionality reduction principal component analysis pca distributed stochastic neighbor embedding sne association rule learning apriori
Methodology,choose classifier,bias variance model le likely overfit example naive bayes bias variance model express complex relationship example logistic regression
Methodology,explain lda unsupervised learning,latent dirichlet allocation lda common method topic modeling generative representing document combination topic probability distribution lda aim higher dimensional space onto lower dimensional space help avoid curse dimensionality
Methodology,fix variance,variance bias bagging algorithm divide subset randomized sampling sample generate model single learning algorithm additionally regularization technique higher coefficient penalized lower complexity overall
EDA,working dataset important variable,remove correlated variable selecting important variable random forest plot variable importance chart lasso regression linear regression variable value forward selection stepwise selection backward selection
Model,advantage disadvantage neural network,advantage entire rather database parallel processing distributed memory provides accuracy limited disadvantage requires complex processor duration somewhat unknown rely error heavily nature
Model,default method splitting decision tree,default method gini measure impurity particular node essentially calculates probability specific feature classified incorrectly element linked single pure random forest gini preferred isn computationally intensive doesn involve logarithm function
Model,xgboost perform svm,xgboos ensemble method us tree mean improves repeat itself svm linear separator linearly separable svm requires kernel separated limit perfect kernel given dataset
Model,nlp purpose encoder decoder,encoder decoder generate output sequence input sequence make encoder decoder powerful decoder us final encoder initial give decoder encoder extracted input sequence
EDA,scikit true scale feature value vary greatly,machine learning algorithm euclidean distance metric measure distance point range value greatly result
